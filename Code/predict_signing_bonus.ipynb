{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "70e5cbe5-f657-4fca-bf7d-f49a2dcb0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from scipy.optimize import root_scalar\n",
    "from scipy.stats import truncnorm\n",
    "import itertools\n",
    "from itertools import permutations, combinations\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "#!pip install sklearn-contrib-py-earth\n",
    "#from pyearth import Earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ddfa679-799b-4f08-9ae2-8e8eb62fdd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: \n",
    "# number (what draft pick they were)\n",
    "# position\n",
    "# hs (high school indicator)\n",
    "# fv (scouting value)\n",
    "# risk (20/80 scale) \n",
    "# OPTIONAL- more advanced scouting metrics? Worried about overfitting, and \n",
    "\n",
    "# output: \n",
    "# pct of max signing bonus they received \n",
    "# maybe just predict bonus as well, if better performance...\n",
    "\n",
    "# models: \n",
    "# regression, as we want this to be linear \n",
    "# interaction terms, second order terms, regularization \n",
    "# potentially MARS? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a08b2de-8bb5-4f74-8583-a38cf92628a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft data from Baseball America \n",
    "\n",
    "draft_1 = pd.read_csv('data_ba - 2021.csv')\n",
    "draft_2= pd.read_csv('data_ba - 2022.csv')\n",
    "draft_3= pd.read_csv('data_ba - 2023.csv')\n",
    "draft_4= pd.read_csv('data_ba - 2024.csv')\n",
    "\n",
    "# scouting data from Fangrpahs \n",
    "\n",
    "fg_1 = pd.read_csv('data_fg - 2021.csv')\n",
    "fg_2 = pd.read_csv('data_fg - 2022.csv')\n",
    "fg_3 = pd.read_csv('data_fg - 2023.csv')\n",
    "fg_4 = pd.read_csv('data_fg - 2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0e953f69-df64-468a-9136-fdc56f41d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging data: \n",
    "\n",
    "df_1 = pd.merge(draft_1, fg_1, how='inner', on='name')\n",
    "df_2 = pd.merge(draft_2, fg_2, how='inner', on='name') \n",
    "df_3 = pd.merge(draft_3, fg_3, how='inner', on='name')\n",
    "df_4 = pd.merge(draft_4, fg_4, how='inner', on='name') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "16c787d2-4f2b-4486-9c16-b178f78f6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining data\n",
    "\n",
    "training_data = pd.concat([df_1, df_2, df_3], ignore_index=True) # testing data is df_4, 2024\n",
    "\n",
    "# before we get the x and y, we need to one hot encode the position variable: \n",
    "\n",
    "training_data = pd.get_dummies(training_data, columns=['position'],dtype=int)\n",
    "df_4 = pd.get_dummies(df_4, columns=['position'],dtype=int)\n",
    "\n",
    "# x and y from training_data, along with testing x and y from 2024\n",
    "x = training_data[['number','position_1B', 'position_C', 'position_IF', 'position_LHP', 'position_RHP', 'position_SS', 'hs','fv','risk']]\n",
    "y = training_data['pct_max_bonus']\n",
    "\n",
    "x_test = df_4[['number','position_1B', 'position_C', 'position_IF', 'position_LHP', 'position_RHP', 'position_SS', 'hs','fv','risk']]\n",
    "y_test = df_4['pct_max_bonus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c62fab91-0f29-4699-b65e-13e7c4661523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the csvs out for use in the MARS R file \n",
    "\n",
    "training_data.to_csv('training_data')\n",
    "df_4.to_csv('df_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2c164f5-f0c3-4718-aa16-a52f0b7efc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R-squared: 0.7195168859351506\n",
      "Out-of-sample R-squared: 0.5379121064205752\n"
     ]
    }
   ],
   "source": [
    "# fit the linear regression model \n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(x, y)\n",
    "\n",
    "print(\"In-sample R-squared:\", model.score(x,y)) # decent in sample R-squared of 0.72, especially considering the sample size of 437\n",
    "\n",
    "print(\"Out-of-sample R-squared:\", model.score(x_test,y_test)) # bad out of sample R-squared of 0.54, which makes sense. We are over fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4040f248-24b7-4b62-a2c9-98da403c2f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R-squared: 0.8913543810620272\n",
      "Out-of-sample R-squared: 0.7975540054244079\n"
     ]
    }
   ],
   "source": [
    "# fit a linear regression model with squared and interaction terms\n",
    "\n",
    "poly_feats = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "model_2 = make_pipeline(poly_feats, LinearRegression())\n",
    "\n",
    "model_2.fit(x, y)\n",
    "\n",
    "print(\"In-sample R-squared:\", model_2.score(x, y)) # Very strong 0.89 in sample!\n",
    "\n",
    "print(\"Out-of-sample R-squared:\", model_2.score(x_test, y_test)) # now we do a very strong 0.79 out of sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dd69f504-4dab-472e-853c-528a9ac130b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R-squared: 0.8898580528881019\n",
      "Out-of-sample R-squared: 0.8089494795608855\n"
     ]
    }
   ],
   "source": [
    "# now, we do it with a Lasso regularizer term \n",
    "\n",
    "poly_feats = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "lasso = Lasso(alpha=0.0001, max_iter=10000)\n",
    "\n",
    "model_3 = make_pipeline(poly_feats,scaler, lasso)\n",
    "\n",
    "model_3.fit(x, y)\n",
    "\n",
    "print(\"In-sample R-squared:\", model_3.score(x, y)) \n",
    "\n",
    "print(\"Out-of-sample R-squared:\", model_3.score(x_test, y_test)) # OOS of 0.57... worse than full model, likely just use full model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6b56067c-7e4d-4f31-98de-293a3bfdb5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R-squared: 0.7195168856738287\n",
      "Out-of-sample R-squared: 0.537920011326827\n"
     ]
    }
   ],
   "source": [
    "#now we try it with ridge regression \n",
    "\n",
    "poly_feats = PolynomialFeatures(degree=1, include_bias=False)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "ridge = Ridge(alpha=.01)\n",
    "\n",
    "model_4 = make_pipeline(poly_feats,scaler, ridge)\n",
    "\n",
    "model_4.fit(x, y)\n",
    "\n",
    "print(\"In-sample R-squared:\", model_4.score(x, y)) \n",
    "\n",
    "print(\"Out-of-sample R-squared:\", model_4.score(x_test, y_test)) # OOS of 0.54... similar to lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a98daf7d-ed72-4c18-9dbe-78cc6d90efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Feature   Coefficient\n",
      "59                       hs^2 -1.092830e-01\n",
      "7                          hs -1.092830e-01\n",
      "29               position_C^2 -9.670931e-02\n",
      "2                  position_C -9.670931e-02\n",
      "34              position_C hs -7.855607e-02\n",
      "5                position_RHP -7.345817e-02\n",
      "50             position_RHP^2 -7.345817e-02\n",
      "6                 position_SS -6.503813e-02\n",
      "55              position_SS^2 -6.503813e-02\n",
      "52            position_RHP hs -5.283639e-02\n",
      "44             position_LHP^2  5.241347e-02\n",
      "4                position_LHP  5.241347e-02\n",
      "20              position_1B^2  4.048519e-02\n",
      "1                 position_1B  4.048519e-02\n",
      "56             position_SS hs -3.571040e-02\n",
      "41             position_IF hs -3.026973e-02\n",
      "26             position_1B hs  2.610479e-02\n",
      "8                          fv -2.548053e-02\n",
      "47            position_LHP hs -2.051676e-02\n",
      "37              position_IF^2  7.186360e-03\n",
      "3                 position_IF  7.186360e-03\n",
      "60                      hs fv  7.068989e-03\n",
      "35              position_C fv  6.326563e-03\n",
      "0                      number  4.722949e-03\n",
      "53            position_RHP fv  4.182580e-03\n",
      "9                        risk -3.626086e-03\n",
      "57             position_SS fv  2.571745e-03\n",
      "48            position_LHP fv -2.234523e-03\n",
      "27             position_1B fv -1.896613e-03\n",
      "36            position_C risk -1.182539e-03\n",
      "43           position_IF risk -6.745174e-04\n",
      "62                       fv^2  5.707100e-04\n",
      "17                  number hs  4.173877e-04\n",
      "16         number position_SS  4.051934e-04\n",
      "54          position_RHP risk -4.039159e-04\n",
      "13         number position_IF  2.804286e-04\n",
      "42             position_IF fv -2.546111e-04\n",
      "49          position_LHP risk -2.232765e-04\n",
      "15        number position_RHP  2.028844e-04\n",
      "18                  number fv -1.961603e-04\n",
      "61                    hs risk -1.885429e-04\n",
      "11         number position_1B  1.501769e-04\n",
      "14        number position_LHP -1.329335e-04\n",
      "63                    fv risk  1.153656e-04\n",
      "58           position_SS risk -7.911813e-05\n",
      "12          number position_C -3.750112e-05\n",
      "28           position_1B risk -3.021101e-05\n",
      "64                     risk^2 -8.894984e-06\n",
      "10                   number^2  4.997760e-06\n",
      "19                number risk  4.744832e-06\n",
      "22    position_1B position_IF  2.386112e-15\n",
      "24   position_1B position_RHP -1.111958e-15\n",
      "23   position_1B position_LHP  6.457508e-16\n",
      "21     position_1B position_C -2.801578e-16\n",
      "25    position_1B position_SS  1.162265e-16\n",
      "33     position_C position_SS  6.245005e-17\n",
      "31    position_C position_LHP -4.163336e-17\n",
      "30     position_C position_IF  3.469447e-17\n",
      "39   position_IF position_RHP -2.428613e-17\n",
      "32    position_C position_RHP -1.973248e-17\n",
      "38   position_IF position_LHP -1.387779e-17\n",
      "45  position_LHP position_RHP  0.000000e+00\n",
      "46   position_LHP position_SS  0.000000e+00\n",
      "40    position_IF position_SS  0.000000e+00\n",
      "51   position_RHP position_SS  0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# look at the features for model 2 (chosen model)\n",
    "\n",
    "pd.set_option('display.max_rows', None) # so that we print everything\n",
    "\n",
    "df = pd.DataFrame({'Feature': features, 'Coefficient': coefs})\n",
    "df_sorted = df.reindex(df['Coefficient'].abs().sort_values(ascending=False).index)\n",
    "print(df_sorted) # most important first \n",
    "\n",
    "# EDA results: high school "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fe197d02-4a8e-44d0-affd-efabc7f03f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset of features: ('number', 'position_IF', 'hs', 'fv', 'risk')\n",
      "Adjusted R²: 0.7146051264182003\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          pct_max_bonus   R-squared:                       0.718\n",
      "Model:                            OLS   Adj. R-squared:                  0.715\n",
      "Method:                 Least Squares   F-statistic:                     219.3\n",
      "Date:                Tue, 17 Jun 2025   Prob (F-statistic):          5.43e-116\n",
      "Time:                        23:54:48   Log-Likelihood:                 365.51\n",
      "No. Observations:                 437   AIC:                            -719.0\n",
      "Df Residuals:                     431   BIC:                            -694.5\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -0.8893      0.058    -15.353      0.000      -1.003      -0.775\n",
      "number         -0.0007   7.94e-05     -8.715      0.000      -0.001      -0.001\n",
      "position_IF    -0.0289      0.015     -1.990      0.047      -0.057      -0.000\n",
      "hs              0.0675      0.012      5.614      0.000       0.044       0.091\n",
      "fv              0.0276      0.001     21.216      0.000       0.025       0.030\n",
      "risk            0.0003      0.000      1.160      0.247      -0.000       0.001\n",
      "==============================================================================\n",
      "Omnibus:                       79.864   Durbin-Watson:                   1.149\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              138.445\n",
      "Skew:                           1.066   Prob(JB):                     8.65e-31\n",
      "Kurtosis:                       4.750   Cond. No.                     1.45e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.45e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "Out-of-sample R-squared: 0.5297939800943134\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def best_subsets(x, y, max_features=None):\n",
    "    if max_features is None:\n",
    "        max_features = x.shape[1]\n",
    "\n",
    "    results = []\n",
    "    for k in range(1, max_features + 1):\n",
    "        for combo in itertools.combinations(x.columns, k):\n",
    "            X_subset = sm.add_constant(x[list(combo)])\n",
    "            model = sm.OLS(y, X_subset).fit()\n",
    "            results.append({\n",
    "                'features': combo,\n",
    "                'adj_r2': model.rsquared_adj,\n",
    "                'model': model\n",
    "            })\n",
    "    \n",
    "    # Get best model by adjusted R²\n",
    "    best = max(results, key=lambda r: r['adj_r2'])\n",
    "    return best\n",
    "\n",
    "# Run best subsets\n",
    "best_model_result = best_subsets(x, y)\n",
    "\n",
    "# Print summary\n",
    "print(\"Best subset of features:\", best_model_result['features'])\n",
    "print(\"Adjusted R²:\", best_model_result['adj_r2'])\n",
    "print(best_model_result['model'].summary())\n",
    "\n",
    "# Get test data with the same best subset of features\n",
    "X_test_subset = sm.add_constant(x_test[list(best_model_result['features'])])\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = best_model_result['model'].predict(X_test_subset)\n",
    "\n",
    "# Calculate out-of-sample R² manually\n",
    "ss_res = np.sum((y_test - y_pred) ** 2)\n",
    "ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "r2_out_of_sample = 1 - ss_res / ss_tot\n",
    "\n",
    "print(\"Out-of-sample R-squared:\", r2_out_of_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
